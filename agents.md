Construyendo un Chat con React, Node/Express, Postgres y el SDK de OpenAI Agents



Integración del SDK oficial de OpenAI Agents en React

OpenAI ofrece un SDK de Agents para TypeScript diseñado para construir aplicaciones con agentes de IA de forma fácil y robusta. Para integrarlo en React, se deben seguir estos pasos básicos:
	•	Instalación: Agrega el paquete @openai/agents a tu proyecto React. Nota: Actualmente requiere una versión específica de Zod (≤3.25.67) debido a una dependencia, según la documentación oficial ￼. Por ejemplo:

npm install @openai/agents zod@3.25.67


	•	Configuración del agente: Utiliza las clases proporcionadas por el SDK para crear un agente. Un agente representa el modelo de lenguaje (p. ej. GPT-4) con ciertas instrucciones iniciales (prompt del sistema) y herramientas disponibles. Un ejemplo mínimo de configuración de un agente y ejecución de una consulta sería:

import { Agent, run } from '@openai/agents';

const agent = new Agent({
  name: 'Assistant',
  instructions: 'You are a helpful assistant.',
});

const result = await run(agent, 'Escribe un haiku sobre la recursión.');
console.log(result.finalOutput);

Este código crea un agente llamado “Assistant” con instrucciones de ser un asistente útil, luego corre una consulta de ejemplo y muestra la respuesta final ￼ ￼. El SDK se encarga internamente del loop de agente: llama a las herramientas necesarias, envía resultados al modelo y repite hasta obtener una respuesta final ￼.

	•	Uso en el navegador: El SDK de Agents está diseñado para TypeScript/Node, pero soporta casos de uso en navegador. De hecho, incluye funcionalidades realtime para agentes de voz que conectan micrófono y audio vía WebRTC en el navegador ￼. Para utilizarlo en el frontend, deberás proporcionar tu clave de API de OpenAI al agente. Una forma segura de hacerlo es establecer la clave en variables de entorno durante el build (por ejemplo usando Vite/CRA .env para inyectarla en import.meta.env), o mejor aún, realizar las llamadas de API desde el backend para no exponer la clave en el cliente. Ten en cuenta que todo lo que se ejecuta en el navegador es potencialmente visible; si haces la llamada directamente desde el frontend, es trivial capturar tu API key ￼. Por ello, aunque técnicamente podrías conectar el agente en el frontend usando session.connect({ apiKey: 'tu-api-key' }), es más seguro enviar las peticiones a través de tu servidor Node (ver sección de backend). Esto previene filtraciones de la clave y apenas impacta en la latencia ￼. Además, se puede habilitar streaming para mejorar la inmediatez percibida por el usuario, ya que el cuello de botella principal será el tiempo de respuesta del modelo y no tanto la red ￼.
	•	Características del SDK: OpenAI Agents SDK proporciona abstracciones mínimas pero poderosas. Permite definir tools (funciones utilitarias que el agente puede llamar, incluso con validación de parámetros vía Zod), guardrails (validaciones de entradas/salidas), handoffs (delegar tareas entre múltiples agentes), y tiene soporte de tracing para depurar flujos ￼. Muchos de estos conceptos te permitirán ampliar la funcionalidad de tu chatbot (por ejemplo, agregando herramientas de cálculo, búsquedas web, etc.), manteniendo la conversación en el frontend sin lógica de negocio en el servidor.

Mantenimiento del estado de la conversación y contexto

Los modelos de OpenAI no conservan estado entre peticiones, por lo que es necesario que la aplicación maneje el historial de la conversación y lo envíe en cada solicitud para mantener el contexto ￼. En esta arquitectura, podemos combinar almacenamiento en el cliente y en la base de datos para lograrlo:
	•	Estado en frontend: A medida que el usuario y el asistente intercambian mensajes, la aplicación React puede mantener un array de mensajes en memoria (por ejemplo, usando un estado con useState o una librería de estado global). Cada mensaje incluiría al menos el rol (user o assistant), el contenido, y quizá un ID de conversación. Al enviar una nueva pregunta al modelo, se toma este historial y se envía junto con la nueva entrada. El SDK de Agents permite pasar directamente mensajes previos como parte del prompt (similar a la API de chat completions). Por ejemplo, podrías usar run(agent, [ ...mensajesPrevios, { role: 'user', content: preguntaActual } ]) para incluir el historial.
	•	Almacenamiento en Postgres: Para persistir conversaciones (ya sea para mostrar historiales pasados al usuario o para mantener contexto entre sesiones), es recomendable guardar cada interacción en la base de datos ￼. Una estrategia común es tener una tabla de conversaciones (con ID, usuario, timestamp, etc.) y otra de mensajes (cada mensaje con referencia a la conversación, rol, contenido y fecha). Cada vez que llega un nuevo mensaje del usuario, puedes: 1) guardarlo en Postgres, 2) recuperar los mensajes recientes de esa conversación, y 3) enviarlos al modelo para obtener la respuesta, 4) guardar la respuesta también en la DB. De esta manera, tienes un registro completo y durable del diálogo. Tip: No es necesario usar caches complicados como Redis a menos que tengas altísimos volúmenes; una base relacional bien indexada puede manejar miles de usuarios con historiales moderados ￼.
	•	Contexto y límite de tokens: Debes ser cuidadoso de no mandar un historial demasiado grande al modelo, ya que hay límites de tokens por petición (p. ej., ~4096 tokens en GPT-3.5, más en GPT-4) ￼. Enviar todo el historial siempre puede ser ineficiente y costoso en tokens ￼. Unas mejores prácticas para mantener contexto sin desbordar el límite incluyen ￼:
	•	Ventana deslizante de mensajes recientes: incluir solo los N últimos mensajes de la conversación que sean relevantes. Por ejemplo, conservar los últimos ~10-20 intercambios recientes y omitir los más antiguos ￼.
	•	Resúmenes de contexto: resumir automáticamente los mensajes antiguos en una descripción corta y añadir ese resumen como parte del contexto (quizá como un mensaje del asistente que condensa la charla previa) ￼ ￼. Podrías usar el propio modelo para generar resúmenes periódicamente (aunque esto consume algunas llamadas adicionales).
	•	Vectorización para memoria larga: para conversaciones muy extensas o bases de conocimiento adicionales, almacenar embeddings de segmentos de la conversación en una base vectorial (ej. pgvector en Postgres, Pinecone, etc.) permite buscar y recuperar dinámicamente las partes más relevantes del historial ￼. Este es un enfoque avanzado para memoria a largo plazo sin enviar todo el texto cada vez.
	•	Nueva conversación (reset): Implementar un botón de “Nuevo chat” simplemente iniciará una conversación vacía sin contexto. En la app React, esto significa limpiar el estado de mensajes actual (y quizás generar un nuevo ID de conversación). En la base de datos, puedes crear un nuevo registro de conversación o generar un nuevo identificador para separar el nuevo hilo del anterior. Así, el próximo mensaje del usuario no incluirá mensajes previos, logrando un reinicio completo del contexto. Esto respeta la preferencia del usuario de “conservar el contexto pero con opción a iniciar de nuevo” – es decir, por defecto se mantiene la conversación continua, pero con un clic se puede comenzar desde cero.

Carga de archivos en el chat

Un requisito mencionado es permitir que el usuario cargue archivos en la conversación, de modo que la IA pueda utilizarlos (por ejemplo, para responder preguntas basadas en un documento PDF o datos proporcionados). El manejo de archivos tiene dos consideraciones: cómo subirlos sin recargar el servidor de almacenamiento, y cómo hacer que el modelo los aproveche.
	•	Subida directa desde el navegador (opción sin servidor): Idealmente, querríamos que el archivo vaya directamente del navegador a la API de OpenAI, sin pasar por nuestro servidor, para no ocupar almacenamiento ni ancho de banda en nuestro backend. Sin embargo, esto depende de que OpenAI provea un endpoint para subida de archivos que podamos invocar desde frontend. OpenAI recientemente introdujo un File Search tool integrado con su API de Responses/Assistants, el cual permite consultar archivos previamente cargados a un vector store gestionado por OpenAI ￼. En teoría, podrías usar las APIs de OpenAI para: (a) crear un vector store y subir el archivo allí, y (b) luego permitir que el agente use esa herramienta. Workaround: Actualmente, el SDK de Agents no expone métodos directos para manejo de archivos locales, así que tendrías que hacer la llamada de carga “manualmente” con fetch o usando el SDK REST de OpenAI (por ejemplo, el cliente oficial openai para Node) desde el frontend. No obstante, esto vuelve a implicar exponer la API key en el cliente, lo cual es un riesgo de seguridad ￼. Por lo tanto, aunque la ruta sea “navegador → OpenAI” directamente, probablemente debas implementar al menos un pequeño servicio o usar una Cloud Function con la clave para efectuar la carga de forma segura.
	•	Carga vía backend (opción segura): La alternativa es manejar la subida en tu servidor Node/Express. Puedes usar un middleware como multer para recibir el archivo en una ruta /upload de tu API. Desde allí, tienes dos caminos:
	1.	Almacenar y usar interno: Guardar temporalmente el archivo (o solo en memoria), procesarlo si es necesario (por ejemplo, leer texto de un PDF) y luego utilizar su contenido para responder. Este método podría implicar extraer texto y pasarlo al modelo dentro del prompt (si el archivo es pequeño) o insertar datos en tu propia base vectorial (como pgvector en Postgres) para consultas de IA. Por ejemplo, podrías indexar el contenido y luego, cuando el usuario haga una pregunta, buscar oraciones relevantes en la DB y agregarlas al prompt del modelo.
	2.	Integración con OpenAI File Search: Utilizar la API de OpenAI para almacenar el archivo en sus servidores de búsqueda vectorial. OpenAI cobra $0.10/GB por día de almacenamiento y $2.50 por cada 1000 consultas de búsqueda en ese vector store ￼, pero a cambio te permite realizar RAG (búsqueda de información relevante) de forma sencilla. El flujo sería: tu backend recibe el archivo y llama a la API de OpenAI para guardarlo en un vector store (obteniendo un vector_store_id único). Luego, al invocar al agente, le pasas una herramienta configurada con type: "file_search" y ese ID de vector store. El modelo entonces puede buscar en el contenido del archivo cuando formule sus respuestas. Según un ejemplo público, basta con indicar el tipo file_search y el ID de tu vector store en el parámetro tools de la petición, y la herramienta incorporada hará el resto ￼. No necesitas implementar funciones adicionales para buscar en el archivo; el modelo recibirá automáticamente anotaciones con los fragmentos relevantes encontrados y los usará para componer la respuesta ￼ ￼. Nota: Esta funcionalidad proviene del Responses API reciente; asegúrate de que tu modelo (p. ej. gpt-4 o gpt-4-32k en su versión para Responses API) soporte herramientas. En el SDK de Agents, configurar esto podría requerir definir la herramienta en el agente o al momento de la solicitud. Si el SDK oficial todavía no soporta la creación/carga de archivos en vector stores a través de métodos directos, puedes emplear la librería oficial de OpenAI para Node (openai npm package) en el backend como solución ￼.
	•	Procesamiento de archivos en frontend: Si por “carga de archivos en el navegador” te refieres también a procesar el archivo del lado del cliente (por ejemplo, leer un PDF/CSV directamente en el navegador para evitar enviarlo al servidor), esto es posible con la API FileReader de JavaScript. Podrías, por ejemplo, leer el texto de un archivo .txt o .pdf en el cliente y enviarlo como parte del prompt. Sin embargo, enviar texto muy grande directamente al modelo puede ser inviable por límites de tokens. Por lo tanto, esta estrategia funciona solo para archivos pequeños o fragmentos. Para PDFs más extensos, conviene resumirlos o subirlos a un vector store como mencionamos. Si necesitas admitir imágenes en las conversaciones, ten en cuenta que la API de OpenAI actualmente no soporta análisis de imágenes vía API estándar (GPT-4 con visión no está abierto generalmene). Tendrías que usar un servicio de OCR externo o una herramienta específica. Dado que el SDK oficial no incluye OCR, podrías sugerir bibliotecas adicionales como Tesseract.js para OCR de imágenes, o pdf.js/pdf-parse para extraer texto de PDFs en Node, si surge esa necesidad (solo si tu aplicación lo requiere y el SDK no lo cubre).

En resumen, la forma recomendada es realizar la carga de archivos a través del backend por seguridad, aprovechando luego el File Search de OpenAI para evitar almacenar datos pesados en tu servidor. Esto ahorra almacenamiento local y delega la indexación al servicio de OpenAI, manteniendo la arquitectura ligera en el frontend.

Uso del backend (Node/Express) y la base de datos Postgres

Aunque gran parte de la interacción con la IA se hará en el frontend, tu backend con Express sigue siendo importante para mantener buenas prácticas de seguridad y persistencia:
	•	Proxy de API OpenAI: Como se señaló, es muy recomendable que las llamadas a OpenAI pasen por el backend. Esto significa exponer endpoints en Express como /api/chat que reciban la pregunta del usuario (y quizás un ID de conversación) y, en el servidor, utilicen la clave de API (almacenada en variables de entorno) para llamar a OpenAI. Esto protege la clave secreta del lado del servidor ￼. El backend simplemente reenvía la pregunta junto con el contexto necesario y retorna la respuesta de la IA al frontend. La latencia añadida por este salto extra es mínima comparada con el tiempo que tarda el modelo en responder, así que la seguridad prevalece sobre la micro-optimización en este caso ￼. Si quieres mejorar la sensación de velocidad, habilita streaming: la API de OpenAI permite obtener respuestas token a token. Puedes implementar un Stream o Server-Sent Events (SSE) en Express para enviar los tokens progresivamente al cliente en cuanto lleguen ￼. De hecho, en un tutorial oficial se sugiere streaming como la mejor vía para feedback inmediato al usuario, manteniendo el backend en la ecuación ￼.
	•	Endpoints para historial: Tu backend también puede exponer rutas para guardar y obtener el historial de conversaciones desde Postgres. Por ejemplo:
	•	POST /api/chat – guarda un nuevo mensaje de usuario en la DB, llama a OpenAI (vía SDK o API) y devuelve la respuesta, guardando también la respuesta en la DB.
	•	GET /api/history?convId=XYZ – recupera todos los mensajes de una conversación dada (para mostrarla al usuario, o para re-hidratar el estado del frontend al recargar la página).
	•	POST /api/new-conversation – crea un nuevo registro de conversación (opcional, pues también se puede crear implícitamente al enviar el primer mensaje de un nuevo chat).
Estas rutas actúan como una fina capa de persistencia y transporte; la lógica de IA sigue residiendo en el agente/SDK utilizado.
	•	Interacción con Postgres: Desde Node, puedes usar un cliente de Postgres (como pg npm) o un ORM (como Prisma, Sequelize, Drizzle, etc.) para almacenar los datos. Las mejores prácticas incluyen parametrizar consultas para prevenir inyecciones SQL, y almacenar solo la información necesaria (el texto de los mensajes y metadatos como usuario, timestamps, etc.). Dado que el usuario mencionó “guardar estados, conversaciones o todo el historial según mejores prácticas”, insistimos en que guardar todo el historial es válido siempre que se maneje el tamaño (p. ej., archivar o resumir muy antiguas). Incluso se podría implementar un borrado automático o archivado de conversaciones muy viejas por privacidad o cumplimiento.
	•	Reiniciar conversaciones: Cuando el frontend indica iniciar un nuevo chat, puedes opcionalmente notificar al backend (p. ej., para que marque la conversación anterior como cerrada o inicie una nueva en la DB). Sin embargo, esto puede manejarse también totalmente del lado cliente si no necesitas registrar ese evento. Por simplicidad, podrías crear la nueva conversación en cuanto el usuario envía el primer mensaje tras pulsar “nuevo chat”.
	•	SDK oficial en backend: Cabe señalar que el OpenAI Agents SDK también puede usarse en Node directamente (no solo en React). De hecho, fue concebido inicialmente para backend/Node usage y acaba de ser lanzado para TypeScript en julio 2025 ￼. Tienes la opción de usarlo en el servidor para orquestar agentes y herramientas ahí, enviando al cliente solo resultados finales. No obstante, según los requisitos dados (“solo en frontend, sin acciones en backend”), es preferible mantener el backend sencillo. Podrías limitarte al cliente OpenAI REST oficial (openai npm) en el backend, que es muy ligero, para hacer las llamadas directas. Esto es compatible con usar el SDK de Agents en el frontend para cosas como reconocimiento de voz en tiempo real u otras interacciones ricas, mientras que el backend actúa más como un proxy y almacenamiento.

Conclusiones y consideraciones finales

En esta arquitectura full-stack se combinan las fortalezas de cada componente: un frontend React interactivo que utiliza el SDK oficial de OpenAI Agents para una integración fluida con la IA, un backend Node/Express ligero que protege credenciales y conserva datos, y PostgreSQL como fuente de verdad para el historial conversacional. Asegúrate de utilizar el SDK oficial siempre que sea posible para las funcionalidades centrales (chat, herramientas integradas, streaming), ya que ofrece un marco soportado por OpenAI y con primitivas sólidas para agentes ￼. Si alguna característica deseada no está disponible en el SDK (por ejemplo, manipulación de archivos o OCR, gestión específica de memoria a largo plazo, etc.), no dudes en incorporar librerías especializadas como complemento. Por ejemplo, podrías usar pgvector con Postgres para almacenar embeddings si necesitas memoria extensa, o librerías de procesamiento de documentos (PDF, imágenes) si quieres extraer contenido de archivos adjuntos antes de enviarlos al modelo. Estas adiciones se pueden hacer manteniendo el núcleo de la solución en el SDK oficial.


